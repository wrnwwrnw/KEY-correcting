# README

## Overview
This repository provides an experimental implementation inspired by **SnapKV** for accelerating large language model inference with KV cache management.  
⚠️ **Note:** This is **not the final version** of the code.

## Environment
The project requires the following key dependencies:
- `transformers==4.57.1`  
- `flash-attn==2.5.8`  
